{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import ampligraph\n",
    "# import rdflib\n",
    "# import itertools\n",
    "\n",
    "#  https://docs.ampligraph.org/en/1.3.1/tutorials/AmpliGraphBasicsTutorial.html\n",
    "#  https://docs.ampligraph.org/en/1.3.1/examples.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r triples_arr\n",
    "%store -r unseen_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29759\n"
     ]
    }
   ],
   "source": [
    "X = triples_arr\n",
    "\n",
    "#  Sanity check \n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1063\n"
     ]
    }
   ],
   "source": [
    "#  Sanity check \n",
    "print(len(unseen_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size:  (24959, 3)\n",
      "Test set size:  (2400, 3)\n",
      "Valid set size:  (2400, 3)\n"
     ]
    }
   ],
   "source": [
    "# Train, test, valid split \n",
    "\n",
    "from ampligraph.evaluation import train_test_split_no_unseen \n",
    "X_train_valid, X_test = train_test_split_no_unseen(X, test_size=(2400))\n",
    "X_train, X_valid = train_test_split_no_unseen(X_train_valid, test_size=2400)\n",
    "\n",
    "print('Train set size: ', X_train.shape)\n",
    "print('Test set size: ', X_test.shape)\n",
    "print('Valid set size: ', X_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define ComplEx model \n",
    "\n",
    "from ampligraph.latent_features import ComplEx\n",
    "\n",
    "model = ComplEx(batches_count=100, \n",
    "                seed=555, \n",
    "                epochs=100, \n",
    "                k=200, \n",
    "                eta=15,\n",
    "                loss='multiclass_nll', \n",
    "                embedding_model_params = {'negative_corruption_entities': 'all'},\n",
    "                regularizer='LP', \n",
    "                regularizer_params={'p':1, 'lambda':1e-5}, \n",
    "                initializer= 'xavier', \n",
    "                initializer_params= {'uniform': False},\n",
    "                optimizer= 'adam',\n",
    "                optimizer_params = {'lr': 0.0005}, \n",
    "                verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss:   0.104632: 100%|███████████████████████████████████████████████████| 100/100 [19:00<00:00, 11.40s/epoch]\n"
     ]
    }
   ],
   "source": [
    "#  Fit model on the training data \n",
    "\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "model.fit(X_train, early_stopping = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  save model \n",
    "\n",
    "from ampligraph.latent_features import save_model\n",
    "save_model(model, './best_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.latent_features import restore_model\n",
    "model = restore_model('./best_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is fit!\n"
     ]
    }
   ],
   "source": [
    "#  Sanity check \n",
    "\n",
    "if model.is_fitted:\n",
    "    print('The model is fit!')\n",
    "else:\n",
    "    print('The model is not fit! Did you skip a step?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING - DeprecationWarning: use_default_protocol will be removed in future. Please use corrupt_side argument instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2400/2400 [03:44<00:00, 10.68it/s]\n"
     ]
    }
   ],
   "source": [
    "#  Evaluate performance on the test set \n",
    "\n",
    "positives_filter = np.concatenate((X_train, X_test, X_valid))\n",
    "\n",
    "from ampligraph.evaluation import evaluate_performance\n",
    "ranks = evaluate_performance(X_test, \n",
    "                             model=model, \n",
    "                             filter_triples=positives_filter,   # Corruption strategy filter defined above \n",
    "                             use_default_protocol=True, # corrupt subj and obj separately while evaluating\n",
    "                             verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR: 332.93\n",
      "MRR: 0.67\n",
      "Hits@10: 0.74\n",
      "Hits@3: 0.68\n",
      "Hits@1: 0.63\n"
     ]
    }
   ],
   "source": [
    "#  Evaluation scores\n",
    "\n",
    "from ampligraph.evaluation import mr_score, mrr_score, hits_at_n_score\n",
    "\n",
    "mr = mr_score(ranks)\n",
    "print(\"MR: %.2f\" % (mr))\n",
    "\n",
    "mrr = mrr_score(ranks)\n",
    "print(\"MRR: %.2f\" % (mrr))\n",
    "\n",
    "hits_10 = hits_at_n_score(ranks, n=10)\n",
    "print(\"Hits@10: %.2f\" % (hits_10))\n",
    "hits_3 = hits_at_n_score(ranks, n=3)\n",
    "print(\"Hits@3: %.2f\" % (hits_3))\n",
    "hits_1 = hits_at_n_score(ranks, n=1)\n",
    "print(\"Hits@1: %.2f\" % (hits_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting new links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  unseen triples from pandas dataframe to numpy array \n",
    "X_unseen = np.array(unseen_triples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_filter = np.array(list({tuple(i) for i in np.vstack((positives_filter, X_unseen))}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_unseen))\n",
    "print(len(unseen_filter)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#  calculate ranks \n",
    "\n",
    "ranks_unseen = evaluate_performance(\n",
    "    X_unseen, \n",
    "    model=model, \n",
    "    filter_triples=unseen_filter,   # Corruption strategy filter defined above \n",
    "    corrupt_side = 's+o',\n",
    "    use_default_protocol=False, # corrupt subj and obj separately while evaluating\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  calculate scores\n",
    "\n",
    "scores = model.predict(X_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Calibrate model on the valid set \n",
    "#  https://docs.ampligraph.org/_/downloads/en/1.3.1/pdf/  See page 27 for explanation  \n",
    "\n",
    "model.calibrate(X_valid, positive_base_rate=0.5)\n",
    "\n",
    "probas_pos_neg = model.predict_proba(X_unseen) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Create DataFrame with ranks, scores and probabilities\n",
    "\n",
    "rankings = pd.DataFrame(list(zip([' '.join(x) for x in X_unseen], \n",
    "                      ranks_unseen, \n",
    "                      np.squeeze(scores),\n",
    "                      np.squeeze(probas_pos_neg))), \n",
    "             columns=['statement', 'rank', 'score', 'probas_pos_neg']).sort_values('probas_pos_neg', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#  inspect the scores \n",
    "\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "pd.set_option('max_rows', 350)\n",
    "rankings = rankings.reset_index(drop=True)\n",
    "rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  store the rankings \n",
    "\n",
    "%store rankings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
